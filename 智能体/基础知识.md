# 项目介绍
技术焦点正从训练更大的基础模型，转向构建更聪明的智能体应用。<br/>
Hello_Agents项目，是Datawhale 社区的系统性智能体学习教程，为社区提供一本从零开始、理论与实战并重的智能体系统构建指南。<br/>
如今 Agent 构建主要分为两派，一派是 Dify，Coze，n8n 这类软件工程类 Agent，其本质是流程驱动的软件开发，LLM 作为数据处理的后端；另一派则是 AI 原生的 Agent，即真正以 AI 驱动的 Agent。<br/>


# 前言
智能体---让 AI 不仅仅是一个“有问必答”的工具，而是成为一个能自主规划、调用工具、解决复杂问题的“行动者”。<br/>
单个智能体已经能胜任特定领域的任务，而由多个智能体分工、协作、甚至辩论，共同完成一个宏大目标的多智能体系统（Multi-Agent System, MAS），则被视为释放 LLM 全部潜能、解决真实世界复杂问题的关键钥匙。<br/>

# 智能体与语言模型基础

## 第一章 初识智能体
### 什么是智能体
在人工智能领域，智能体被定义为任何能够通过传感器（Sensors）感知其所处环境（Environment），并自主地通过执行器（Actuators）采取行动（Action）以达成特定目标的实体。<br/>
这个定义包含了智能体存在的四个基本要素：<br/>
（1）环境：环境是智能体所处的外部世界。对于自动驾驶汽车，环境是动态变化的道路交通；对于一个交易算法，环境则是瞬息万变的金融市场。智能体并非与环境隔离，它通过其传感器持续地感知环境状态。<br/>
（2）传感器：摄像头、麦克风、雷达或各类应用程序编程接口（Application Programming Interface, API）返回的数据流，都是其感知能力的延伸。<br/>

（3）执行器：获取信息后，智能体需要采取行动来对环境施加影响，它通过执行器来改变环境的状态。执行器可以是物理设备（如机械臂、方向盘）或虚拟工具（如执行一段代码、调用一个服务）。<br/>

（4）行动：然而，真正赋予智能体"智能"的，是其自主性（Autonomy）。智能体并非只是被动响应外部刺激或严格执行预设指令的程序，它能够基于其感知和内部状态进行独立决策，以达成其设计目标。这种从感知到行动的闭环，构成了所有智能体行为的基础。<br/>


#### 传统视角下的智能体
起点：①反射智能体。<br/>
决策核心由工程师明确设计的“条件-动作”规则构成。经典的自动恒温器便是如此：若传感器感知的室温高于设定值，则启动制冷系统。这种智能体完全依赖于当前的感知输入，不具备记忆或预测能力。<br/>

发展：②基于模型的反射智能体（Model-Based Reflex Agent）。<br/>
这类智能体拥有一个内部的世界模型（World Model），用于追踪和理解环境中那些无法被直接感知的方面。它试图回答：“世界现在是什么样子的？”。例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。这个内部模型让智能体拥有了初级的“记忆”，使其决策不再仅仅依赖于瞬时感知，而是基于一个更连贯、更完整的世界状态理解。<br/>

发展：③基于目标的智能体（Goal-Based Agent）的发展。<br/>
主动地、有预见性地选择能够导向某个特定未来状态的行动。这类智能体需要回答的问题是：“我应该做什么才能达成目标？”。经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。这类智能体的核心能力体现在了对未来的考量与规划上。<br/>

发展：④当多个目标需要权衡时，基于效用的智能体（Utility-Based Agent）便随之出现。<br/>
它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。它需要回答一个更复杂的问题：“哪种行为能为我带来最满意的结果？”。这种架构让智能体学会在相互冲突的目标之间进行权衡，使其决策更接近人类的理性选择。<br/>

以上四个的核心决策逻辑，无论是规则、模型还是效用函数，依然依赖于人类设计师的先验知识。<br/>

发展：⑤学习型智能体（Learning Agent）的核心思想是不依赖预设，而是通过与环境的互动自主学习。<br/>
而强化学习（Reinforcement Learning, RL）是实现这一思想最具代表性的路径。一个学习型智能体包含一个性能元件（即我们前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。<br/>

想象一个学习下棋的 AI。它开始时可能只是随机落子，当它最终赢下一局时，系统会给予它一个正向的奖励。通过大量的自我对弈，学习元件会逐渐发现哪些棋路更有可能导向最终的胜利。AlphaGo Zero 是这一理念的一个里程碑式的成就。它在围棋这一复杂博弈中，通过强化学习发现了许多超越人类既有知识的有效策略。<br/>


#### 大语言模型驱动的新范式
由大语言模型驱动的 LLM 智能体，其核心决策机制与传统智能体存在本质区别，从而赋予了其一系列全新的特性。
简而言之，传统智能体的能力源于工程师的显式编程与知识构建，其行为模式是确定且有边界的；而 LLM 智能体则通过在海量数据上的预训练，获得了隐式的世界模型与强大的涌现能力，使其能够以更灵活、更通用的方式应对复杂任务。
！[传统智能体与 LLM 驱动智能体的核心对比.png](images/传统智能体与%20LLM%20驱动智能体的核心对比.png)